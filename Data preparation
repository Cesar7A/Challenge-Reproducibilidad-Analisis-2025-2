#import

import pyspark
from pyspark.sql import functions as F
from pyspark.sql.functions import col, count
from pyspark import SparkConf
from pyspark.sql import SparkSession
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pyspark.sql.types import (StructType, 
                               StructField, 
                               DateType, 
                               BooleanType,
                               DoubleType,
                               IntegerType,
                               StringType,
                               TimestampType)

#construct schema
flight_schema = StructType([StructField("legId", StringType(), True),
                            StructField("searchDate", StringType(), True),
                            StructField("flightDate", DateType(), True),
                            StructField("startingAirport", StringType(), True),
                            StructField("destinationAirport", StringType(), True),
                            StructField("fareBasisCode", StringType(), True),
                            StructField("travelDuration", StringType(), True),
                            StructField("elapsedDays", IntegerType(), True ),
                            StructField("isBasicEconomy", BooleanType(), True),
                            StructField("isRefundable", BooleanType(), True),
                            StructField("isNonStop", BooleanType(), True),
                            StructField("baseFare", DoubleType(), True),
                            StructField("totalFare", DoubleType(), True),
                            StructField("seatsRemaining", IntegerType(), True),
                            StructField("totalTravelDistance", IntegerType(), True)
                           
                            ])
data_path = "/kaggle/input/flightprices/itineraries.csv"

sc = pyspark.SparkContext(appName="price_predict")

 #read dataset

spark = SparkSession \
    .builder \
    .appName("intro_to_spark") \
    .getOrCreate()
data1 = spark.read.format("csv")\
  .option("header", "true")\
  .schema(flight_schema)\
  .load(data_path)

data1.printSchema()
data1.limit(3).toPandas()
data = data1.sample(fraction=0.01)  # Select 60% of the rows randomly
#data=data1.sample(frac=0.1)
data.count()

#chech null values
data.filter(col("totalTravelDistance").isNull()).count()

from pyspark.sql.functions import mean, col
distance_mean = data.select(mean(col('totalTravelDistance'))).collect()[0][0]

# Fill the NAs in the 'Age' column with the mean value
data = data.fillna({'totalTravelDistance': distance_mean})

# import airport_zipcode dataset
airport_zip_path = "/kaggle/input/airport2/airport_data.txt"
airport_zip_schema= StructType([
    StructField("ICAO Code", StringType(), True),
    StructField("IATA Code", StringType(), True),
    StructField("Airport Name", StringType(), True),
    StructField("City/Town", StringType(), True),
    StructField("Country", StringType(), True),
    StructField("Latitude Degrees", IntegerType(), True),
    StructField("Latitude Minutes", IntegerType(), True),
    StructField("Latitude Seconds", IntegerType(), True),
    StructField("Latitude Direction", StringType(), True),
    StructField("Longitude Degrees", IntegerType(), True),
    StructField("Longitude Minutes", IntegerType(), True),
    StructField("Longitude Seconds", IntegerType(), True),
    StructField("Longitude Direction", StringType(), True),
    StructField("Altitude", IntegerType(), True),
    StructField("Latitude Decimal Degrees", DoubleType(), True),
    StructField("Longitude Decimal Degrees", DoubleType(), True)
])

airport_zipcode = spark.read.format("csv")\
  .option("header", "true")\
  .schema(airport_zip_schema)\
  .load(airport_zip_path)

#join the flight_price and airport_zipcode
flight_airport = data.join(airport_zipcode, data.startingAirport == airport_zipcode['IATA Code'], 'left_outer')


